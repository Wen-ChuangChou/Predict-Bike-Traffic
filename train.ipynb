{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAT network for finding bike traffic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/797981034638231270', creation_time=1725170517251, experiment_id='797981034638231270', last_update_time=1725170517251, lifecycle_stage='active', name='Predict bike traffic_GAT_fixed_floor_area', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Specify tracking server\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Predict bike traffic_GAT_fixed_floor_area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import momepy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "# import setuptools\n",
    "import torch\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from shapely import LineString\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "# from torch_geometric.transforms import NormalizeScale\n",
    "\n",
    "from utils.config import SIGNATURE\n",
    "\n",
    "\n",
    "class OptimizerType(Enum):\n",
    "    ADAM = {\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0.001,\n",
    "    }\n",
    "    SGD = {\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"momentum\": 0.8,\n",
    "        \"weight_decay\": 0.0001,\n",
    "    }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"num_hidden_channels\": 24,\n",
    "    \"GNN_dropout_rate\": 0.2,\n",
    "    \"head_num\": 12,\n",
    "    \"optimizer\": OptimizerType.ADAM,  # or OptimizerType.SGD\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_size\": 128,\n",
    "    #  the learning rate of each parameter group using a cosine annealing schedule\n",
    "    \"scheduler\": False\n",
    "    # \"scheduler_gamma\": [0.8],\n",
    "    # \"pos_weight\": [1.3],\n",
    "    # \"model_embedding_size\": [64],\n",
    "    # \"model_attention_heads\": [3],\n",
    "    # \"model_layers\": [4],\n",
    "    # \"model_dropout_rate\": [0.2],\n",
    "    # \"model_top_k_ratio\": [0.5],\n",
    "    # \"model_top_k_every_n\": [1],\n",
    "    # \"model_dense_neurons\": [256]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from E:\\MyDoc\\Deep_Thinkvitality\\MLcode\\Bike_Prediction\\data\\processed\\Dresden_testF4mean.gpkg\n",
      "Data(edge_index=[2, 55050], x=[12912, 43], y=[12912, 1])\n"
     ]
    }
   ],
   "source": [
    "prefix_name = \"testF4mean\"\n",
    "place_name = \"Dresden\"\n",
    "file_name = place_name + \"_\" + prefix_name\n",
    "\n",
    "len_normalize = 100\n",
    "\n",
    "data_path = Path().resolve() / \"data\" / \"processed\" / f\"{file_name}.gpkg\"\n",
    "traffic = gpd.read_file(data_path, driver='GPKG', layer='traffic')\n",
    "print(f\"loading data from {data_path}\")\n",
    "\n",
    "# traffic.crs = \"epsg:4326\"\n",
    "traffic = traffic.to_crs(4839)\n",
    "# traffic.crs = \"epsg:3857\"\n",
    "\n",
    "loaded_graph = momepy.gdf_to_nx(traffic, approach=\"primal\")\n",
    "nodes = max(nx.connected_components(loaded_graph), key=len)\n",
    "connected_graph = nx.subgraph(loaded_graph, nodes)\n",
    "connected_graph = nx.Graph(connected_graph)\n",
    "# H_nodes, H_edges = momepy.nx_to_gdf(H)\n",
    "# print(H_edges.head(12))\n",
    "\n",
    "num_idx_graph = nx.convert_node_labels_to_integers(\n",
    "    connected_graph, label_attribute=\"coordinate\")\n",
    "node_loc = {\n",
    "    i: num_idx_graph.nodes[i][\"coordinate\"]\n",
    "    for i in range(num_idx_graph.number_of_nodes())\n",
    "}\n",
    "\n",
    "lineGraph = nx.line_graph(num_idx_graph)\n",
    "# for node in lineGraph.copy():\n",
    "#     lineGraph.add_node(node,mm_len=num_idx_graph.edges[node][\"mm_len\"]/len_normalize,\n",
    "#                 occurrence=num_idx_graph.edges[node][\"occurrence\"],\n",
    "#                 bikeability=num_idx_graph.edges[node][\"bikeability\"]\n",
    "#                 )\n",
    "\n",
    "attr_list = [\n",
    "    \"mm_len\", \"occurrence\", \"bikeability\", \"floor_area\", \"department_store\",\n",
    "    \"supermarket\", \"kiosk\", \"variety_store\", \"foodstore\", \"bakery\",\n",
    "    \"ice_cream\", \"bookstore\", \"ticket\", \"copyshop\", \"fashion\", \"DIY\",\n",
    "    \"houseware\", \"furniture\", \"electronics\", \"sportstore\", \"florist\",\n",
    "    \"laundry\", \"petstore\", \"toystore\", \"cafe\", \"restaurant\", \"pub\", \"theatre\",\n",
    "    \"cinema\", \"market\", \"place_of_worship\", \"bank\", \"pharmacy\", \"chemist\",\n",
    "    \"post_office\", \"townhall\", \"library\", \"kindergarten\", \"school\", \"college\",\n",
    "    \"park\", \"stadium\", \"sportplace\", \"university\"\n",
    "]\n",
    "attr_list_no_occurrence = [\n",
    "    \"mm_len\", \"bikeability\", \"floor_area\", \"department_store\", \"supermarket\",\n",
    "    \"kiosk\", \"variety_store\", \"foodstore\", \"bakery\", \"ice_cream\", \"bookstore\",\n",
    "    \"ticket\", \"copyshop\", \"fashion\", \"DIY\", \"houseware\", \"furniture\",\n",
    "    \"electronics\", \"sportstore\", \"florist\", \"laundry\", \"petstore\", \"toystore\",\n",
    "    \"cafe\", \"restaurant\", \"pub\", \"theatre\", \"cinema\", \"market\",\n",
    "    \"place_of_worship\", \"bank\", \"pharmacy\", \"chemist\", \"post_office\",\n",
    "    \"townhall\", \"library\", \"kindergarten\", \"school\", \"college\", \"park\",\n",
    "    \"stadium\", \"sportplace\", \"university\"\n",
    "]\n",
    "for node in lineGraph.copy():\n",
    "    attr_dict = {\n",
    "        attr_name: num_idx_graph.edges[node][attr_name]\n",
    "        for attr_name in attr_list\n",
    "    }\n",
    "    lineGraph.add_node(node, **attr_dict)\n",
    "\n",
    "data = from_networkx(lineGraph, group_node_attrs=attr_list_no_occurrence)\n",
    "data.x[:, 0] = data.x[:, 0] / len_normalize\n",
    "# data.x = F.normalize(data.x, dim=0)\n",
    "# data.x = z_score(data.x)\n",
    "data.y = data.occurrence.unsqueeze(1)\n",
    "del data.occurrence\n",
    "# data = transform(data)\n",
    "# type(pyg_graph)\n",
    "print(data)\n",
    "# print(nx.number_of_edges(lineGraph))\n",
    "# L2.edges\n",
    "# print(pyg_graph.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the traffic flow in designated areas\n",
    "\n",
    "# _, gdf_edges = momepy.nx_to_gdf(connected_graph)\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "# gdf_edges.plot(column=\"occurrence\",\n",
    "#                ax=ax,\n",
    "#                legend=True,\n",
    "#                legend_kwds={\"shrink\": 0.5},\n",
    "#                norm=colors.LogNorm(vmin=1, vmax=gdf_edges[\"occurrence\"].max()))\n",
    "# ax.set_axis_off()\n",
    "\n",
    "# fig_path = Path().resolve() / \"data\" / \"image\"\n",
    "# fig_path.mkdir(parents=True, exist_ok=True)\n",
    "# plt.savefig(fig_path / f\"{file_name}.svg\", format=\"svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find histogram of features\n",
    "# idx = 0\n",
    "# test = data.x[:,idx]\n",
    "# plt.hist(test,bins=20,range=(0, 5))\n",
    "# print(attr_list_no_occurrence[idx])\n",
    "# print(max(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_edges.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to trains and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "time_tag = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "data.train_mask = torch.rand(data.num_nodes) < params[\"train_ratio\"]\n",
    "data.test_mask = ~data.train_mask\n",
    "mask_path = Path().resolve() / \"data\" / \"mask\"\n",
    "mask_path.mkdir(parents=True, exist_ok=True)\n",
    "np.save(mask_path / f\"{file_name}_{time_tag}.npy\", data.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (convs): ModuleList(\n",
      "    (0): GATConv(43, 24, heads=12)\n",
      "    (1): GATConv(288, 24, heads=12)\n",
      "    (2): GATConv(288, 24, heads=1)\n",
      "  )\n",
      "  (linear1): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (linear2): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (linear3): Linear(in_features=24, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear, MSELoss\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv, TopKPooling\n",
    "\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_layers, hidden_channels, heads,\n",
    "                 dropout):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        # GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(num_features, hidden_channels,\n",
    "                                  heads))  # First layer\n",
    "\n",
    "        for _ in range(1, num_layers - 1):  # Middle layers\n",
    "            self.convs.append(\n",
    "                GATConv(hidden_channels * heads, hidden_channels, heads))\n",
    "\n",
    "        self.convs.append(\n",
    "            GATConv(hidden_channels * heads,\n",
    "                    hidden_channels,\n",
    "                    heads=1,\n",
    "                    concat=False))  # Last layer\n",
    "\n",
    "        # linear layers\n",
    "        self.linear1 = Linear(hidden_channels, 24)\n",
    "        self.linear2 = Linear(24, 24)\n",
    "        self.linear3 = Linear(24, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Iterate over GNN layers\n",
    "        for _, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # linear layer\n",
    "        x = self.linear1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GAT(data.num_features, params[\"num_layers\"],\n",
    "            params[\"num_hidden_channels\"], params[\"head_num\"],\n",
    "            params[\"GNN_dropout_rate\"])\n",
    "print(model)\n",
    "# print(f\"Number of parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1:\n",
      "=======\n",
      "Number of nodes in the current batch: 3222\n",
      "Data(x=[3222, 43], y=[3222, 1], train_mask=[3222], test_mask=[3222], edge_index=[2, 9940])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of nodes in the current batch: 3230\n",
      "Data(x=[3230, 43], y=[3230, 1], train_mask=[3230], test_mask=[3230], edge_index=[2, 10042])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of nodes in the current batch: 3225\n",
      "Data(x=[3225, 43], y=[3225, 1], train_mask=[3225], test_mask=[3225], edge_index=[2, 9480])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of nodes in the current batch: 3235\n",
      "Data(x=[3235, 43], y=[3235, 1], train_mask=[3235], test_mask=[3235], edge_index=[2, 9418])\n",
      "\n",
      "Iterated over 12912 of 12912 nodes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "cluster_data = ClusterData(data, num_parts=512)  # 1. Create subgraphs. 128\n",
    "train_loader = ClusterLoader(\n",
    "    cluster_data, batch_size=params[\"batch_size\"],\n",
    "    shuffle=True)  # 2. Stochastic partioning scheme. 32\n",
    "\n",
    "print()\n",
    "total_num_nodes = 0\n",
    "for step, sub_data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n",
    "    print(sub_data)\n",
    "    print()\n",
    "    total_num_nodes += sub_data.num_nodes\n",
    "\n",
    "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "\n",
    "    for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "\n",
    "        sub_data = sub_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(sub_data.x, sub_data.edge_index)\n",
    "\n",
    "        # Only use nodes with labels available for loss calculation --> mask\n",
    "        loss = criterion(out[sub_data.train_mask],\n",
    "                         sub_data.y[sub_data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "    return running_loss / step\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Function to create a frame for the animation\n",
    "def create_frame(epoch):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    truth = data.y.detach().numpy()\n",
    "    prediction = prediction.detach().numpy()\n",
    "    pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "    max_ground_value = max(data.y.detach().numpy())\n",
    "    mask = data.train_mask.numpy()\n",
    "\n",
    "    plt.plot(pair_predict[mask, 0],\n",
    "             pair_predict[mask, 1],\n",
    "             'o',\n",
    "             color='royalblue',\n",
    "             markersize=2,\n",
    "             label='train data')\n",
    "    plt.plot(pair_predict[~mask, 0],\n",
    "             pair_predict[~mask, 1],\n",
    "             'o',\n",
    "             color='indianred',\n",
    "             markersize=2,\n",
    "             label='test data')\n",
    "\n",
    "    plt.text(0.05,\n",
    "             0.95,\n",
    "             f'Epoch: {epoch:04d}',\n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize='medium',\n",
    "             verticalalignment='top')\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.xlim([0, max_ground_value])\n",
    "    plt.ylim([0, max_ground_value])\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    return plt.gca().figure,\n",
    "\n",
    "\n",
    "# Create the animation\n",
    "def create_animation(max_epoch):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ani = FuncAnimation(fig,\n",
    "                        create_frame,\n",
    "                        frames=range(max_epoch),\n",
    "                        repeat=False)\n",
    "\n",
    "    # Save the animation\n",
    "    # animation_path = output_path / f\"{file_name}_{time_tag}.mp4\"\n",
    "    animation_path = Path().resolve() / \"data\" / \"anima\"\n",
    "    animation_path.mkdir(parents=True, exist_ok=True)\n",
    "    ani.save(animation_path / f\"{file_name}_{time_tag}.gif\",\n",
    "             writer='ffmpeg',\n",
    "             fps=5)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def animation():\n",
    "    animation_count = 0\n",
    "    max_groud_value = max(data.y.detach().numpy())\n",
    "    mask = data.train_mask.numpy()\n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    truth = data.y.detach().numpy()\n",
    "    prediction = prediction.detach().numpy()\n",
    "    pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "    animation_path = Path().resolve()/\"data\"/\"anima\"/\\\n",
    "            f\"{file_name}_{time_tag}_{animation_count:03d}.png\"\n",
    "\n",
    "    scatter1 = plt.plot(pair_predict[mask.transpose().ravel(), 0],\n",
    "                        pair_predict[mask.transpose().ravel(), 1],\n",
    "                        'o',\n",
    "                        color='royalblue',\n",
    "                        markersize=2,\n",
    "                        label='train data')\n",
    "    scateer2 = plt.plot(pair_predict[~mask.transpose().ravel(), 0],\n",
    "                        pair_predict[~mask.transpose().ravel(), 1],\n",
    "                        'o',\n",
    "                        color='indianred',\n",
    "                        markersize=2,\n",
    "                        label='test data')\n",
    "\n",
    "    plt.text(-300, -300, f'epoch: {epoch:04d}', fontsize='medium')\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.xlim([0, max_groud_value])\n",
    "    plt.ylim([0, max_groud_value])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(animation_path, format=\"png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    animation_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize variables for early stopping\n",
    "max_epoch = 5000\n",
    "best_test_loss = float('inf')  # Set to infinity initially\n",
    "patience_counter = 20\n",
    "early_stopping_patience = 500  # Number of epochs with no improvement to wait before stopping\n",
    "log_after_epochs = 30  # Start logging the model only after this number of epochs\n",
    "animatiing_results = False\n",
    "\n",
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "optimizer_type = params[\"optimizer\"]\n",
    "optimizer_config = optimizer_type.value\n",
    "\n",
    "if params[\"optimizer\"] == OptimizerType.ADAM:\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=optimizer_config[\"learning_rate\"],\n",
    "                                 betas=optimizer_config[\"betas\"],\n",
    "                                 eps=optimizer_config[\"eps\"],\n",
    "                                 weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up Adam optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'betas: {optimizer_config[\"betas\"]}, '\n",
    "          f'eps: {optimizer_config[\"eps\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "elif params[\"optimizer\"] == OptimizerType.SGD:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=optimizer_config[\"learning_rate\"],\n",
    "                                momentum=optimizer_config[\"momentum\"],\n",
    "                                weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up SGD optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'momentum: {optimizer_config[\"momentum\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "\n",
    "if params[\"scheduler\"] == True:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                           T_max=500,\n",
    "                                                           eta_min=0.000001)\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Define loss function\n",
    "criterion = MSELoss()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    mlflow.log_param(\"dataset\", file_name)\n",
    "    mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "    # mlflow.log_artifact(fig_path)\n",
    "    mlflow.log_artifact(mask_path / f\"{file_name}_{time_tag}.npy\")\n",
    "\n",
    "    # Log the general parameters\n",
    "    for key in params:\n",
    "        if key != \"optimizer\":  # Exclude logging the optimizer enum object directly\n",
    "            mlflow.log_param(key, params[key])\n",
    "\n",
    "    # Log the optimizer type\n",
    "    mlflow.log_param(\"optimizer_type\", optimizer_type.name)\n",
    "\n",
    "    # Log each parameter within the optimizer config\n",
    "    for key, value in optimizer_config.items():\n",
    "        mlflow.log_param(f\"{optimizer_type.name.lower()}_{key}\", value)\n",
    "    \n",
    "    progress_bar = tqdm(total=max_epoch, desc=\"Train\", unit=\" step\", position=0, leave=True)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(max_epoch):\n",
    "            #train\n",
    "            train_loss = np.sqrt(train())\n",
    "            mlflow.log_metric(key=\"Train loss\",\n",
    "                              value=float(train_loss),\n",
    "                              step=epoch)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #test\n",
    "            test_loss = torch.sqrt(test())\n",
    "            mlflow.log_metric(key=\"Test loss\",\n",
    "                              value=float(test_loss),\n",
    "                              step=epoch)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # progress display\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"train loss\": f\"{train_loss:.2f}\",\n",
    "                \"test loss\": f\"{test_loss:.2f}\"\n",
    "            })\n",
    "            \n",
    "            # Check if the validation loss has improved\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                patience_counter = 0  # Reset patience counter\n",
    "                # Only log the best model after `log_after_epochs` have passed\n",
    "                if epoch >= log_after_epochs:\n",
    "                    # if warnings of mismatching between the model's dependencies \n",
    "                    # and the current Python environment appear, \n",
    "                    # consider creating a suitable requirements.txt\n",
    "                    mlflow.pytorch.log_model(model, \"model\", \n",
    "                                             pip_requirements=\"requirements_local.txt\", \n",
    "                                             signature=SIGNATURE)  # Log the best model\n",
    "                    print(f\"Model logged at epoch {epoch} with test loss: {test_loss:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1  # Increment patience counter\n",
    "            \n",
    "            # Early stopping condition\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best test loss: {best_test_loss:.4f}\")\n",
    "                break\n",
    "            # scheduler.step()\n",
    "            # if epoch % 10 == 0 and animatiing_results:\n",
    "            if animatiing_results:\n",
    "                create_animation(max_epoch)\n",
    "                print(\"Plot one frame\")\n",
    "\n",
    "    finally:\n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Plotting the losses after training\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        \n",
    "        # Plot for training loss\n",
    "        ax1.plot(epochs, train_losses, 'b', label='Train Loss')\n",
    "        ax1.set_title('Training Loss vs Epochs')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot for test loss\n",
    "        ax2.plot(epochs, test_losses, 'r', label='Test Loss')\n",
    "        ax2.set_title('Test Loss vs Epochs')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Display the plots side by side\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    # plot  \n",
    "    result_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}.svg\"\n",
    "    result_Emap_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}_Emap.svg\"\n",
    "    \n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    truth = data.y.detach().cpu().numpy()\n",
    "    prediction = prediction.detach().cpu().numpy()\n",
    "    error = prediction - truth\n",
    "    pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "    max_ground_value = max(data.y.detach().cpu().numpy())\n",
    "    mask = data.train_mask.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(pair_predict[mask, 0],\n",
    "             pair_predict[mask, 1],\n",
    "             'o',\n",
    "             color='royalblue',\n",
    "             markersize=3,\n",
    "             label='train data')\n",
    "    plt.plot(pair_predict[~mask, 0],\n",
    "             pair_predict[~mask, 1],\n",
    "             'o',\n",
    "             color='indianred',\n",
    "             markersize=3,\n",
    "             label='test data')\n",
    "\n",
    "    # plt.plot(pair_predict[:, 0], pair_predict[:, 1], \"o\", markersize=3)\n",
    "    # plt.plot(pair_predict[data.test_mask.cpu(), 0],\n",
    "    #          pair_predict[data.test_mask.cpu(), 1],\n",
    "    #          \"or\",\n",
    "    #          markersize=3)\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.xlim([0, max_ground_value])\n",
    "    plt.ylim([0, max_ground_value])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(result_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    lines = [\n",
    "        LineString([node_loc[node[0]], node_loc[node[1]]])\n",
    "        for node in lineGraph.nodes\n",
    "    ]\n",
    "    readout_gdf = gpd.GeoDataFrame({\n",
    "        \"error\": error.ravel(),\n",
    "        \"geometry\": lines\n",
    "    },\n",
    "                                   crs=\"EPSG:4839\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    readout_gdf.plot(column=\"error\",\n",
    "                     ax=ax,\n",
    "                     legend=True,\n",
    "                     legend_kwds={\"shrink\": 0.4},\n",
    "                    #  norm=colors.LogNorm(vmin=1, vmax=5000)\n",
    "                     )\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(result_Emap_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(result_path)\n",
    "    mlflow.log_artifact(result_Emap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for early stopping\n",
    "max_epoch = 5000\n",
    "best_test_loss = float('inf')  # Set to infinity initially\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 500  # Number of epochs with no improvement to wait before stopping\n",
    "log_after_epochs = 20  # Start logging the model only after this number of epochs\n",
    "animatiing_results = False\n",
    "\n",
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "optimizer_type = params[\"optimizer\"]\n",
    "optimizer_config = optimizer_type.value\n",
    "\n",
    "if params[\"optimizer\"] == OptimizerType.ADAM:\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=optimizer_config[\"learning_rate\"],\n",
    "                                 betas=optimizer_config[\"betas\"],\n",
    "                                 eps=optimizer_config[\"eps\"],\n",
    "                                 weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up Adam optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'betas: {optimizer_config[\"betas\"]}, '\n",
    "          f'eps: {optimizer_config[\"eps\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "elif params[\"optimizer\"] == OptimizerType.SGD:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=optimizer_config[\"learning_rate\"],\n",
    "                                momentum=optimizer_config[\"momentum\"],\n",
    "                                weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up SGD optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'momentum: {optimizer_config[\"momentum\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "\n",
    "if params[\"scheduler\"] == True:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                           T_max=500,\n",
    "                                                           eta_min=0.000001)\n",
    "# Initialize lists to store losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Define loss function\n",
    "criterion = MSELoss()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    mlflow.log_param(\"dataset\", file_name)\n",
    "    mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "    # mlflow.log_artifact(fig_path)\n",
    "    mlflow.log_artifact(mask_path / f\"{file_name}_{time_tag}.npy\")\n",
    "\n",
    "    # Log the general parameters\n",
    "    for key in params:\n",
    "        if key != \"optimizer\":  # Exclude logging the optimizer enum object directly\n",
    "            mlflow.log_param(key, params[key])\n",
    "\n",
    "    # Log the optimizer type\n",
    "    mlflow.log_param(\"optimizer_type\", optimizer_type.name)\n",
    "\n",
    "    # Log each parameter within the optimizer config\n",
    "    for key, value in optimizer_config.items():\n",
    "        mlflow.log_param(f\"{optimizer_type.name.lower()}_{key}\", value)\n",
    "    \n",
    "    progress_bar = tqdm(total=max_epoch, desc=\"Train\", unit=\" step\", position=0, leave=True)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(max_epoch):\n",
    "            #train\n",
    "            train_loss = np.sqrt(train())\n",
    "            mlflow.log_metric(key=\"Train loss\",\n",
    "                              value=float(train_loss),\n",
    "                              step=epoch)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #test\n",
    "            test_loss = torch.sqrt(test())\n",
    "            mlflow.log_metric(key=\"Test loss\",\n",
    "                              value=float(test_loss),\n",
    "                              step=epoch)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # progress display\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"train loss\": f\"{train_loss:.2f}\",\n",
    "                \"test loss\": f\"{test_loss:.2f}\"\n",
    "            })\n",
    "            \n",
    "            # Check if the validation loss has improved\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                patience_counter = 0  # Reset patience counter\n",
    "                # Only log the best model after `log_after_epochs` have passed\n",
    "                if epoch >= log_after_epochs:\n",
    "                    mlflow.pytorch.log_model(model, \"model\", \n",
    "                                             pip_requirements=\"requirements.txt\", \n",
    "                                             signature=SIGNATURE)  # Log the best model\n",
    "                    print(f\"Model logged at epoch {epoch} with test loss: {test_loss:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1  # Increment patience counter\n",
    "            \n",
    "            # Early stopping condition\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best test loss: {best_test_loss:.4f}\")\n",
    "                break\n",
    "            # scheduler.step()\n",
    "            # if epoch % 10 == 0 and animatiing_results:\n",
    "            if animatiing_results:\n",
    "                create_animation(max_epoch)\n",
    "                print(\"Plot one frame\")\n",
    "\n",
    "    finally:\n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Plotting the losses after training\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        \n",
    "        # Plot for training loss\n",
    "        ax1.plot(epochs, train_losses, 'b', label='Train Loss')\n",
    "        ax1.set_title('Training Loss vs Epochs')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot for test loss\n",
    "        ax2.plot(epochs, test_losses, 'r', label='Test Loss')\n",
    "        ax2.set_title('Test Loss vs Epochs')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Display the plots side by side\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    # plot  \n",
    "    result_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}.svg\"\n",
    "    result_Emap_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}_Emap.svg\"\n",
    "    \n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    truth = data.y.detach().cpu().numpy()\n",
    "    prediction = prediction.detach().cpu().numpy()\n",
    "    error = prediction - truth\n",
    "    pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "    max_ground_value = max(data.y.detach().cpu().numpy())\n",
    "    mask = data.train_mask.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(pair_predict[mask, 0],\n",
    "             pair_predict[mask, 1],\n",
    "             'o',\n",
    "             color='royalblue',\n",
    "             markersize=3,\n",
    "             label='train data')\n",
    "    plt.plot(pair_predict[~mask, 0],\n",
    "             pair_predict[~mask, 1],\n",
    "             'o',\n",
    "             color='indianred',\n",
    "             markersize=3,\n",
    "             label='test data')\n",
    "\n",
    "    # plt.plot(pair_predict[:, 0], pair_predict[:, 1], \"o\", markersize=3)\n",
    "    # plt.plot(pair_predict[data.test_mask.cpu(), 0],\n",
    "    #          pair_predict[data.test_mask.cpu(), 1],\n",
    "    #          \"or\",\n",
    "    #          markersize=3)\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.xlim([0, max_ground_value])\n",
    "    plt.ylim([0, max_ground_value])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(result_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    lines = [\n",
    "        LineString([node_loc[node[0]], node_loc[node[1]]])\n",
    "        for node in lineGraph.nodes\n",
    "    ]\n",
    "    readout_gdf = gpd.GeoDataFrame({\n",
    "        \"error\": error.ravel(),\n",
    "        \"geometry\": lines\n",
    "    },\n",
    "                                   crs=\"EPSG:4839\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    readout_gdf.plot(column=\"error\",\n",
    "                     ax=ax,\n",
    "                     legend=True,\n",
    "                     legend_kwds={\"shrink\": 0.4},\n",
    "                    #  norm=colors.LogNorm(vmin=1, vmax=5000)\n",
    "                     )\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(result_Emap_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(result_path)\n",
    "    mlflow.log_artifact(result_Emap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "bin_range1 = (0, np.max(truth))\n",
    "bin_range2 = (0, np.max(prediction))\n",
    "# bin_range2 = (0, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3), sharey=True)\n",
    "axes[0].hist(truth, bins=n_bins, range=bin_range1)\n",
    "axes[0].set_title('Ground truth')\n",
    "axes[0].set_xlim(left=0)\n",
    "\n",
    "axes[1].hist(prediction, bins=n_bins, range=bin_range2)\n",
    "axes[1].set_title('Prediction')\n",
    "axes[1].set_xlim(left=0)\n",
    "\n",
    "# plt.xlim(bin_range)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of groups\n",
    "num_groups = 5\n",
    "max_value = 1000\n",
    "\n",
    "pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "# Split data into groups based on values of the first element\n",
    "grouped_data = [\n",
    "    pair_predict[(pair_predict[:, 0] >= i * max_value / num_groups)\n",
    "                 & (pair_predict[:, 0] < (i + 1) * max_value / num_groups)]\n",
    "    for i in range(num_groups)\n",
    "]\n",
    "\n",
    "# Plot histograms for each group\n",
    "fig, axs = plt.subplots(num_groups, 1, figsize=(6, 12), sharex=True)\n",
    "for i in range(num_groups):\n",
    "    axs[i].hist(grouped_data[i][:, 1], bins=50)\n",
    "    axs[i].set_title(\n",
    "        f'Group {i+1}: {i*1000/num_groups} - {(i+1)*1000/num_groups}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pair_predict[:, 0], pair_predict[:, 1], \"o\", markersize=3)\n",
    "plt.plot(pair_predict[data.test_mask, 0],\n",
    "         pair_predict[data.test_mask, 1],\n",
    "         \"or\",\n",
    "         markersize=3)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.xlim([0, 200])\n",
    "plt.ylim([0, 200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "bin_range = (-400, 400)\n",
    "plt.hist(error, bins=n_bins, range=bin_range)\n",
    "plt.title('Prediction Error')\n",
    "plt.xlim(bin_range)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-bike",
   "language": "python",
   "name": "geo-bike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "49c49ff44ed0ae36360824ee13f261ab546314db5036ad4974ccea6b04329394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
