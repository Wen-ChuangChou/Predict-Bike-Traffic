{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training GAT network with 3 GAT layers + fixed floor area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/252164278876110764', creation_time=1724494965109, experiment_id='252164278876110764', last_update_time=1724494965109, lifecycle_stage='active', name='Predict bike traffic_GAT_fixed_floor_area', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Specify tracking server\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Predict bike traffic_GAT_fixed_floor_area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import geopandas as gpd\n",
    "import momepy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from shapely import LineString\n",
    "import torch\n",
    "# from torch_geometric.transforms import NormalizeScale\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from utils.config import SIGNATURE\n",
    "\n",
    "\n",
    "class OptimizerType(Enum):\n",
    "    ADAM = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0.001,\n",
    "    }\n",
    "    SGD = {\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"momentum\": 0.8,\n",
    "        \"weight_decay\": 0.0001,\n",
    "    }\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"num_hidden_channels\": 12,\n",
    "    \"GNN_dropout_rate\": 0.1,\n",
    "    \"head_num\": 9,\n",
    "    #0:Adam, 1:SGD\n",
    "    \"optimizer\": OptimizerType.ADAM,  # or OptimizerType.SGD  \n",
    "    #  the learning rate of each parameter group using a cosine annealing schedule\n",
    "    \"scheduler\": 0\n",
    "    # \"scheduler_gamma\": [0.8],\n",
    "    # \"pos_weight\": [1.3],\n",
    "    # \"model_embedding_size\": [64],\n",
    "    # \"model_attention_heads\": [3],\n",
    "    # \"model_layers\": [4],\n",
    "    # \"model_dropout_rate\": [0.2],\n",
    "    # \"model_top_k_ratio\": [0.5],\n",
    "    # \"model_top_k_every_n\": [1],\n",
    "    # \"model_dense_neurons\": [256]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from E:\\MyDoc\\Deep_Thinkvitality\\MLcode\\Bike_Prediction\\data\\processed\\Dresden_testF4mean.gpkg\n",
      "Data(edge_index=[2, 55050], x=[12912, 43], y=[12912, 1])\n"
     ]
    }
   ],
   "source": [
    "prefix_name = \"testF4mean\"\n",
    "place_name = \"Dresden\"\n",
    "file_name = place_name + \"_\" + prefix_name\n",
    "\n",
    "len_normalize = 100\n",
    "\n",
    "data_path = Path().resolve() / \"data\" / \"processed\" / f\"{file_name}.gpkg\"\n",
    "traffic = gpd.read_file(data_path, driver='GPKG', layer='traffic')\n",
    "print(f\"loading data from {data_path}\")\n",
    "\n",
    "# traffic.crs = \"epsg:4326\"\n",
    "traffic = traffic.to_crs(4839)\n",
    "# traffic.crs = \"epsg:3857\"\n",
    "\n",
    "loaded_graph = momepy.gdf_to_nx(traffic, approach=\"primal\")\n",
    "nodes = max(nx.connected_components(loaded_graph), key=len)\n",
    "connected_graph = nx.subgraph(loaded_graph, nodes)\n",
    "connected_graph = nx.Graph(connected_graph)\n",
    "# H_nodes, H_edges = momepy.nx_to_gdf(H)\n",
    "# print(H_edges.head(12))\n",
    "\n",
    "num_idx_graph = nx.convert_node_labels_to_integers(\n",
    "    connected_graph, label_attribute=\"coordinate\")\n",
    "node_loc = {\n",
    "    i: num_idx_graph.nodes[i][\"coordinate\"]\n",
    "    for i in range(num_idx_graph.number_of_nodes())\n",
    "}\n",
    "\n",
    "lineGraph = nx.line_graph(num_idx_graph)\n",
    "# for node in lineGraph.copy():\n",
    "#     lineGraph.add_node(node,mm_len=num_idx_graph.edges[node][\"mm_len\"]/len_normalize,\n",
    "#                 occurrence=num_idx_graph.edges[node][\"occurrence\"],\n",
    "#                 bikeability=num_idx_graph.edges[node][\"bikeability\"]\n",
    "#                 )\n",
    "\n",
    "attr_list = [\n",
    "    \"mm_len\", \"occurrence\", \"bikeability\", \"floor_area\", \"department_store\",\n",
    "    \"supermarket\", \"kiosk\", \"variety_store\", \"foodstore\", \"bakery\",\n",
    "    \"ice_cream\", \"bookstore\", \"ticket\", \"copyshop\", \"fashion\", \"DIY\",\n",
    "    \"houseware\", \"furniture\", \"electronics\", \"sportstore\", \"florist\",\n",
    "    \"laundry\", \"petstore\", \"toystore\", \"cafe\", \"restaurant\", \"pub\", \"theatre\",\n",
    "    \"cinema\", \"market\", \"place_of_worship\", \"bank\", \"pharmacy\", \"chemist\",\n",
    "    \"post_office\", \"townhall\", \"library\", \"kindergarten\", \"school\", \"college\",\n",
    "    \"park\", \"stadium\", \"sportplace\", \"university\"\n",
    "]\n",
    "attr_list_no_occurrence = [\n",
    "    \"mm_len\", \"bikeability\", \"floor_area\", \"department_store\", \"supermarket\",\n",
    "    \"kiosk\", \"variety_store\", \"foodstore\", \"bakery\", \"ice_cream\", \"bookstore\",\n",
    "    \"ticket\", \"copyshop\", \"fashion\", \"DIY\", \"houseware\", \"furniture\",\n",
    "    \"electronics\", \"sportstore\", \"florist\", \"laundry\", \"petstore\", \"toystore\",\n",
    "    \"cafe\", \"restaurant\", \"pub\", \"theatre\", \"cinema\", \"market\",\n",
    "    \"place_of_worship\", \"bank\", \"pharmacy\", \"chemist\", \"post_office\",\n",
    "    \"townhall\", \"library\", \"kindergarten\", \"school\", \"college\", \"park\",\n",
    "    \"stadium\", \"sportplace\", \"university\"\n",
    "]\n",
    "for node in lineGraph.copy():\n",
    "    attr_dict = {\n",
    "        attr_name: num_idx_graph.edges[node][attr_name]\n",
    "        for attr_name in attr_list\n",
    "    }\n",
    "    lineGraph.add_node(node, **attr_dict)\n",
    "\n",
    "data = from_networkx(lineGraph, group_node_attrs=attr_list_no_occurrence)\n",
    "data.x[:, 0] = data.x[:, 0] / len_normalize\n",
    "# data.x = F.normalize(data.x, dim=0)\n",
    "# data.x = z_score(data.x)\n",
    "data.y = data.occurrence.unsqueeze(1)\n",
    "del data.occurrence\n",
    "# data = transform(data)\n",
    "# type(pyg_graph)\n",
    "print(data)\n",
    "# print(nx.number_of_edges(lineGraph))\n",
    "# L2.edges\n",
    "# print(pyg_graph.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "_, gdf_edges = momepy.nx_to_gdf(connected_graph)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "gdf_edges.plot(\n",
    "        column=\"occurrence\",\n",
    "        ax=ax,\n",
    "        legend=True, \n",
    "        legend_kwds={\"shrink\": 0.5},\n",
    "        norm=colors.LogNorm(vmin=1, \n",
    "        vmax=gdf_edges[\"occurrence\"].max())\n",
    ")\n",
    "ax.set_axis_off()\n",
    "\n",
    "fig_path = Path().resolve()/\"data\"/\"image\"/f\"{file_name}.svg\"\n",
    "plt.savefig(fig_path, format=\"svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find histogram of features\n",
    "# idx = 0\n",
    "# test = data.x[:,idx]\n",
    "# plt.hist(test,bins=20,range=(0, 5))\n",
    "# print(attr_list_no_occurrence[idx])\n",
    "# print(max(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_edges.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to trains and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "time_tag = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "data.train_mask = torch.rand(data.num_nodes) < params[\"train_ratio\"]\n",
    "data.test_mask = ~data.train_mask\n",
    "train_mask_path = Path().resolve(\n",
    ") / \"data\" / \"mask\" / f\"{file_name}_{time_tag}.npy\"\n",
    "np.save(train_mask_path, data.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(43, 12, heads=9)\n",
      "  (conv2): GATConv(108, 12, heads=9)\n",
      "  (conv3): GATConv(108, 12, heads=1)\n",
      "  (linear1): Linear(in_features=12, out_features=24, bias=True)\n",
      "  (linear2): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (linear3): Linear(in_features=24, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters: 19225\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear, MSELoss\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv, TopKPooling\n",
    "\n",
    "# hidden_channels=16\n",
    "\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        # GNN layers\n",
    "        self.conv1 = GATConv(data.num_features,\n",
    "                             hidden_channels,\n",
    "                             heads=params[\"head_num\"])\n",
    "        self.conv2 = GATConv(hidden_channels * params[\"head_num\"],\n",
    "                             hidden_channels,\n",
    "                             heads=params[\"head_num\"])\n",
    "        self.conv3 = GATConv(hidden_channels * params[\"head_num\"],\n",
    "                             hidden_channels,\n",
    "                             heads=1,\n",
    "                             concat=False)\n",
    "\n",
    "        # linear layers\n",
    "        self.linear1 = Linear(hidden_channels, 24)\n",
    "        self.linear2 = Linear(24, 24)\n",
    "        self.linear3 = Linear(24, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=params[\"GNN_dropout_rate\"], training=self.training)\n",
    "\n",
    "        # Second Message Passing Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=params[\"GNN_dropout_rate\"], training=self.training)\n",
    "\n",
    "        # Third Message Passing Layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=params[\"GNN_dropout_rate\"], training=self.training)\n",
    "\n",
    "        # linear layer\n",
    "        x = self.linear1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GAT(params[\"num_hidden_channels\"])\n",
    "print(model)\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1:\n",
      "=======\n",
      "Number of nodes in the current batch: 3222\n",
      "Data(x=[3222, 43], y=[3222, 1], train_mask=[3222], test_mask=[3222], edge_index=[2, 9940])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of nodes in the current batch: 3230\n",
      "Data(x=[3230, 43], y=[3230, 1], train_mask=[3230], test_mask=[3230], edge_index=[2, 10042])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of nodes in the current batch: 3225\n",
      "Data(x=[3225, 43], y=[3225, 1], train_mask=[3225], test_mask=[3225], edge_index=[2, 9480])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of nodes in the current batch: 3235\n",
      "Data(x=[3235, 43], y=[3235, 1], train_mask=[3235], test_mask=[3235], edge_index=[2, 9418])\n",
      "\n",
      "Iterated over 12912 of 12912 nodes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "cluster_data = ClusterData(data, num_parts=512)  # 1. Create subgraphs. 128\n",
    "train_loader = ClusterLoader(\n",
    "    cluster_data, batch_size=128,\n",
    "    shuffle=True)  # 2. Stochastic partioning scheme. 32\n",
    "\n",
    "print()\n",
    "total_num_nodes = 0\n",
    "for step, sub_data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n",
    "    print(sub_data)\n",
    "    print()\n",
    "    total_num_nodes += sub_data.num_nodes\n",
    "\n",
    "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Adam optimizer with lr: 0.01, betas: (0.9, 0.999), eps: 1e-08, weight_decay: 0.001\n",
      "Epoch: 000, Train Loss: 166.4, Test Loss: 143.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/25 11:38:31 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run melodic-stork-436 at: http://localhost:5000/#/experiments/252164278876110764/runs/30167ff1d2e64493a43aaf78126245b1.\n",
      "2024/08/25 11:38:31 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/252164278876110764.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epoch):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stopping_counter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_early_stopping_counter:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m#train\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    128\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mlog_metric(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m                           value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(train_loss),\n\u001b[0;32m    130\u001b[0m                           step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m#test\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Use all data as input, because all nodes have node features\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# out = model(data.x, data.edge_index)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43msub_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Only use nodes with labels available for loss calculation --> mask\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# loss = criterion(out[data.train_mask], data.y[data.train_mask])\u001b[39;00m\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[0;32m     61\u001b[0m     out[sub_data\u001b[38;5;241m.\u001b[39mtrain_mask], sub_data\u001b[38;5;241m.\u001b[39my[sub_data\u001b[38;5;241m.\u001b[39mtrain_mask]\n\u001b[0;32m     62\u001b[0m )  \u001b[38;5;66;03m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ox\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# First Message Passing Layer (Transformation)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN_dropout_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ox\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ox\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:226\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Next, we compute node-level attention coefficients, both for source\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# and target nodes (if present):\u001b[39;00m\n\u001b[0;32m    225\u001b[0m alpha_src \u001b[38;5;241m=\u001b[39m (x_src \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_src)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m alpha_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x_dst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mx_dst\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt_dst\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (alpha_src, alpha_dst)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_self_loops:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "# mlflow.pytorch.autolog()\n",
    "# model = GCN(hidden_channels)\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "optimizer_type = params[\"optimizer\"]\n",
    "optimizer_config = optimizer_type.value\n",
    "\n",
    "if params[\"optimizer\"] == OptimizerType.ADAM:\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=optimizer_config[\"learning_rate\"],\n",
    "                                 betas=optimizer_config[\"betas\"],\n",
    "                                 eps=optimizer_config[\"eps\"],\n",
    "                                 weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up Adam optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'betas: {optimizer_config[\"betas\"]}, '\n",
    "          f'eps: {optimizer_config[\"eps\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "\n",
    "elif params[\"optimizer\"] == OptimizerType.SGD:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=optimizer_config[\"learning_rate\"],\n",
    "                                momentum=optimizer_config[\"momentum\"],\n",
    "                                weight_decay=optimizer_config[\"weight_decay\"])\n",
    "    print(f'Setting up SGD optimizer with '\n",
    "          f'lr: {optimizer_config[\"learning_rate\"]}, '\n",
    "          f'momentum: {optimizer_config[\"momentum\"]}, '\n",
    "          f'weight_decay: {optimizer_config[\"weight_decay\"]}')\n",
    "\n",
    "if params[\"scheduler\"] == 1:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                           T_max=500,\n",
    "                                                           eta_min=0.000001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = MSELoss()\n",
    "# criterion = L1Loss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "\n",
    "    for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "        optimizer.zero_grad()\n",
    "        # Use all data as input, because all nodes have node features\n",
    "        # out = model(data.x, data.edge_index)\n",
    "        out = model(sub_data.x,\n",
    "                    sub_data.edge_index)  # Perform a single forward pass.\n",
    "\n",
    "        # Only use nodes with labels available for loss calculation --> mask\n",
    "        # loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss = criterion(\n",
    "            out[sub_data.train_mask], sub_data.y[sub_data.train_mask]\n",
    "        )  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "    return running_loss / step\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # rmse = RMSE(data.y[data.test_mask], out[data.test_mask])\n",
    "    loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "    return loss\n",
    "\n",
    "\n",
    "# def RMSE(v, v_):\n",
    "#     \"\"\"\n",
    "#     Mean squared error.\n",
    "#     :param v: torch array, ground truth.\n",
    "#     :param v_: torch array, prediction.\n",
    "#     :return: torch scalar, RMSE averages on all elements of input.\n",
    "#     \"\"\"\n",
    "#     return torch.sqrt(torch.mean((v_ - v) ** 2))\n",
    "# return torch.mean((v_ - v) ** 2)\n",
    "\n",
    "# animation setting\n",
    "do_animation = 0\n",
    "animation_count = 0\n",
    "max_groud_value = max(data.y.detach().numpy())\n",
    "mask = data.train_mask.numpy()\n",
    "\n",
    "max_epoch = 5001\n",
    "max_early_stopping_counter = 500\n",
    "# best_train_loss = 100000\n",
    "best_test_loss = 100000\n",
    "nocheck_until = 3000\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    mlflow.log_param(\"dataset\", file_name)\n",
    "    mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "    # mlflow.log_artifact(fig_path)\n",
    "    mlflow.log_artifact(train_mask_path)\n",
    "\n",
    "    # Log the general parameters\n",
    "    for key in params:\n",
    "        if key != \"optimizer\":  # Exclude logging the optimizer enum object directly\n",
    "            mlflow.log_param(key, params[key])\n",
    "\n",
    "    # Log the optimizer type\n",
    "    mlflow.log_param(\"optimizer_type\", optimizer_type.name)\n",
    "\n",
    "    # Log each parameter within the optimizer config\n",
    "    for key, value in optimizer_config.items():\n",
    "        mlflow.log_param(f\"{optimizer_type.name.lower()}_{key}\", value)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        if early_stopping_counter <= max_early_stopping_counter:\n",
    "            #train\n",
    "            train_loss = np.sqrt(train())\n",
    "            mlflow.log_metric(key=\"Train loss\",\n",
    "                              value=float(train_loss),\n",
    "                              step=epoch)\n",
    "\n",
    "            #test\n",
    "            test_loss = torch.sqrt(test())\n",
    "            mlflow.log_metric(key=\"Test loss\",\n",
    "                              value=float(test_loss),\n",
    "                              step=epoch)\n",
    "\n",
    "            # if epoch > nocheck_until: #do not check early_stopping after some epochs\n",
    "            #       if float(test_loss) < best_test_loss:\n",
    "            #             best_loss = test_loss\n",
    "            #             early_stopping_counter = 0\n",
    "            #             # mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "\n",
    "            #       if float(train_loss) < best_train_loss:\n",
    "            #             best_loss = train_loss\n",
    "            #             early_stopping_counter = 0\n",
    "            #             mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "            #       else:\n",
    "            #             early_stopping_counter += 1\n",
    "\n",
    "            # if epoch > nocheck_until: #do not check early_stopping after some epochs\n",
    "            #       if float(test_loss) < best_loss:\n",
    "            #             best_loss = test_loss\n",
    "            #             early_stopping_counter = 0\n",
    "            #             mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "            #       else:\n",
    "            #             early_stopping_counter += 1\n",
    "\n",
    "            if epoch > nocheck_until:  #do not check early_stopping after some epochs\n",
    "\n",
    "                if float(test_loss) < best_test_loss:\n",
    "                    best_test_loss = test_loss\n",
    "                    mlflow.pytorch.log_model(model,\n",
    "                                             \"model\",\n",
    "                                             signature=SIGNATURE)\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "            # scheduler.step()\n",
    "        else:\n",
    "            print(\"Early stopping due to no improvement.\")\n",
    "            break\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.1f}, Test Loss: {test_loss:.1f}\"\n",
    "            )\n",
    "\n",
    "        if epoch % 10 == 0 and do_animation == 1:\n",
    "            prediction = model(data.x, data.edge_index)\n",
    "            truth = data.y.detach().numpy()\n",
    "            prediction = prediction.detach().numpy()\n",
    "            pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "            animation_path = Path().resolve()/\"data\"/\"anima\"/\\\n",
    "                  f\"{file_name}_{time_tag}_{animation_count:03d}.png\"\n",
    "\n",
    "            scatter1 = plt.plot(pair_predict[mask.transpose().ravel(), 0],\n",
    "                                pair_predict[mask.transpose().ravel(), 1],\n",
    "                                'o',\n",
    "                                color='royalblue',\n",
    "                                markersize=2,\n",
    "                                label='train data')\n",
    "            scateer2 = plt.plot(pair_predict[~mask.transpose().ravel(), 0],\n",
    "                                pair_predict[~mask.transpose().ravel(), 1],\n",
    "                                'o',\n",
    "                                color='indianred',\n",
    "                                markersize=2,\n",
    "                                label='test data')\n",
    "\n",
    "            plt.text(-300, -300, f'epoch: {epoch:04d}', fontsize='medium')\n",
    "            plt.xlabel(\"Ground Truth\")\n",
    "            plt.ylabel(\"Prediction\")\n",
    "            plt.xlim([0, max_groud_value])\n",
    "            plt.ylim([0, max_groud_value])\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.savefig(animation_path, format=\"png\", dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "            animation_count += 1\n",
    "\n",
    "    # mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "\n",
    "    # plot\n",
    "\n",
    "    result_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}.svg\"\n",
    "    result_Emap_path = Path().resolve(\n",
    "    ) / \"data\" / \"image\" / f\"{file_name}_{time_tag}_Emap.svg\"\n",
    "\n",
    "    prediction = model(data.x, data.edge_index)\n",
    "    truth = data.y.detach().numpy()\n",
    "    prediction = prediction.detach().numpy()\n",
    "    error = prediction - truth\n",
    "    pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "    plt.plot(pair_predict[:, 0], pair_predict[:, 1], \"o\", markersize=3)\n",
    "    plt.plot(pair_predict[data.test_mask, 0],\n",
    "             pair_predict[data.test_mask, 1],\n",
    "             \"or\",\n",
    "             markersize=3)\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.xlim([0, max(pair_predict[:, 0])])\n",
    "    plt.ylim([0, max(pair_predict[:, 0])])\n",
    "    plt.savefig(result_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    lines = [\n",
    "        LineString([node_loc[node[0]], node_loc[node[1]]])\n",
    "        for node in lineGraph.nodes\n",
    "    ]\n",
    "    readout_gdf = gpd.GeoDataFrame({\n",
    "        \"error\": error.ravel(),\n",
    "        \"geometry\": lines\n",
    "    },\n",
    "                                   crs=\"EPSG:4839\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    readout_gdf.plot(\n",
    "        column=\"error\",\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        legend_kwds={\"shrink\": 0.4},\n",
    "        # norm=colors.LogNorm(vmin=1, vmax=1200)\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(result_Emap_path, format=\"svg\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(result_path)\n",
    "    mlflow.log_artifact(result_Emap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "bin_range1 = (0, np.max(truth))\n",
    "bin_range2 = (0, np.max(prediction))\n",
    "# bin_range2 = (0, 200)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(6,3), sharey=True)\n",
    "axes[0].hist(truth, bins=n_bins, range=bin_range1)\n",
    "axes[0].set_title('Ground truth')\n",
    "axes[0].set_xlim(left=0)\n",
    "\n",
    "axes[1].hist(prediction, bins=n_bins, range=bin_range2)\n",
    "axes[1].set_title('Prediction')\n",
    "axes[1].set_xlim(left=0)\n",
    "\n",
    "# plt.xlim(bin_range)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of groups\n",
    "num_groups = 5\n",
    "max_value = 1000\n",
    "\n",
    "pair_predict = np.concatenate((truth, prediction), axis=1)\n",
    "\n",
    "# Split data into groups based on values of the first element\n",
    "grouped_data = [pair_predict[(pair_predict[:, 0] >= i * max_value / num_groups) & \n",
    "                             (pair_predict[:, 0] < (i + 1) * max_value / num_groups)] for i in range(num_groups)]\n",
    "\n",
    "# Plot histograms for each group\n",
    "fig, axs = plt.subplots(num_groups, 1, figsize=(6, 12), sharex=True)\n",
    "for i in range(num_groups):\n",
    "    axs[i].hist(grouped_data[i][:, 1], bins=50)\n",
    "    axs[i].set_title(f'Group {i+1}: {i*1000/num_groups} - {(i+1)*1000/num_groups}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pair_predict[:,0], pair_predict[:,1], \"o\", markersize=3)\n",
    "plt.plot(pair_predict[data.test_mask,0], pair_predict[data.test_mask,1], \"or\", markersize=3)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.xlim([0, 200])\n",
    "plt.ylim([0, 200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "bin_range = (-400, 400)\n",
    "plt.hist(error, bins=n_bins, range=bin_range)\n",
    "plt.title('Prediction Error')\n",
    "plt.xlim(bin_range)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49c49ff44ed0ae36360824ee13f261ab546314db5036ad4974ccea6b04329394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
